<div id="top" align="center">

<p align="center">
  <img src="asset/caption.jpg" alt="WholeBodyVLA Logo">
</p>

**Towards Unified Latent VLA for Whole-body Loco-manipulation Control**

[![Paper](https://img.shields.io/badge/ArXiv-Coming_Soon-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](#)
[![Home](https://img.shields.io/badge/page-project-5F259F?style=for-the-badge&logo=homepage&logoColor=white)](https://opendrivelab.com/wholebodyvla) 

</div>

> ‚úíÔ∏è Haoran Jiang*, Jin Chen*, Qingwen Bu, Li Chen, Modi Shi, Yanjie Zhang, Delong Li, Chuanzhe Suo, Chuang Wang, Zhihui Peng<sup>‚Ä†</sup>, Hongyang Li<sup>‚Ä†</sup>
> 
> üìß Primary Contact: Haoran Jiang ([jianghaoran2024@gmail.com](mailto:jianghaoran2024@gmail.com)).


## üî• Highlights

- A Vision-Language-Action framework for closed-loop humanoid loco-manipulation control in large space.
- A novel approach for learning unified latent actions from manipulation and manipulation-aware locomotion videos without action annotations.
- A locomotion-oriented reinforcement learning policy that enables precise and stable whole-body coordination under disturbances.

## üìã Overview

WholeBodyVLA is a unified Vision-Language-Action framework for large-space humanoid loco-manipulation. It learns unified latent actions from action-free egocentric videos through a Latent Action Model (LAM), and employs a loco-manipulation-oriented (LMO) RL policy for precise and stable whole-body coordination. The system encodes egocentric images and language instructions into latent action tokens, which are decoded into dual-arm joint actions and locomotion commands, enabling end-to-end control for complex loco-manipulation tasks.

<div align="center">
  <img src="asset/method.png" alt="WholeBodyVLA Method" width="90%">
</div>

<div align="center">
  <img src="asset/long2.gif" width="32%">
  <img src="asset/coffee.gif" width="32%">
  <img src="asset/chair.gif" width="32%">
</div>

**See more on [project website](https://opendrivelab.com/wholebodyvla).**

üìù **Note:** We currently have no plans to open-source the codebase. This repository now serves as a collection of resources and references for the whole-body humanoid VLA research community. We welcome discussion and collaboration!



---

**Let's go for VLA on humanoids!**


# Awesome Vision‚ÄìLanguage‚ÄìAction for Humanoid Robots

*Coming soon...*



A curated list of research on **Vision‚ÄìLanguage‚ÄìAction (VLA)** models for humanoid robots, with a focus on **loco-manipulation**.


## High-Level Reasoning & Planning for Humanoids


### Vision‚ÄìLanguage‚ÄìBased Task Planning


- (paper) ...


### Planning with Motion Capture


- (paper) ...


### Hierarchical and Modular Design


- (paper) ...


### Motion Generation


- (paper) ...


## Whole-Body Controller for Loco-Manipulation


### Behavior Foundation Models


- (paper) ...


### Universal Whole-Body Tracking

- (paper) ...


### Velocity Tracking

- (paper) ...


## Hardware Setup, Teleoperation & Data Collection


- (paper) ...


## Generalist Vision‚ÄìLanguage‚ÄìAction Models


### In-Place Manipulation


- (paper) ...


### Mobile Manipulation 

- (paper) ...


### Navigation

- (paper) ...



<!-- ## Physics-Based Characters Control & Motion Generation


### Motion Generation Model


- (paper) ...
- (paper) ...


### Physics-Based Character Control

Control and reinforcement learning methods for physically simulated humanoid characters with rich contact dynamics.

- (paper) ...
- (paper) ...

---


--- -->


<!-- 
<div align="center">

**Made with ‚ù§Ô∏è by [OpenDriveLab](https://opendrivelab.com/)**

</div> -->
